#!/usr/bin/env python3
"""
ä½¿ç”¨ç¬¬ä¸‰æ–¹APIæ¥å£çˆ¬å–å°çº¢ä¹¦ä»Šæ—¥çƒ­æœæ•°æ®
åŸºäºé¡ºä¸ºæ•°æ®API: https://api.itapi.cn/doc/85

ä½¿ç”¨æ–¹æ³•:
1. æ³¨å†Œé¡ºä¸ºæ•°æ®è´¦å·è·å–APIå¯†é’¥
2. è®¾ç½®ç¯å¢ƒå˜é‡ XIAOHONGSHU_API_KEY æˆ–ç›´æ¥åœ¨ä»£ç ä¸­å¡«å…¥
3. è¿è¡Œè„šæœ¬: python crawl_today_with_api.py
"""
import os
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from crawler.api_crawler import create_api_crawler
from utils.data_saver import DataSaver
from utils.analysis import analyze_hot_search_data
from utils.report_generator import generate_html_report


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ•·ï¸  å°çº¢ä¹¦ä»Šæ—¥çƒ­æœçˆ¬è™«å¯åŠ¨ (APIæ¥å£ç‰ˆ)...")
    
    # è·å–APIå¯†é’¥
    api_key = os.getenv('XIAOHONGSHU_API_KEY')
    if not api_key:
        print("âŒ æœªè®¾ç½®APIå¯†é’¥")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ XIAOHONGSHU_API_KEY")
        print("è·å–APIå¯†é’¥æ–¹æ³•:")
        print("1. è®¿é—® https://api.itapi.cn/doc/85")
        print("2. æ³¨å†Œè´¦å·å¹¶è·å–APIå¯†é’¥")
        print("3. è®¾ç½®ç¯å¢ƒå˜é‡: export XIAOHONGSHU_API_KEY=ä½ çš„å¯†é’¥")
        return 1
    
    try:
        # åˆ›å»ºçˆ¬è™«
        print("æ­£åœ¨åˆ›å»ºAPIçˆ¬è™«...")
        crawler = create_api_crawler(api_key)
        
        # çˆ¬å–æ•°æ®
        print("æ­£åœ¨çˆ¬å–ä»Šæ—¥çƒ­æœæ•°æ®...")
        result = crawler.crawl_hot_search()
        
        if not result or result.total == 0:
            print("âŒ è·å–çƒ­æœæ•°æ®å¤±è´¥")
            return 1
        
        print(f"âœ… æˆåŠŸè·å– {result.total} æ¡çƒ­æœæ•°æ®")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        today = datetime.now().strftime("%Y-%m-%d")
        data_filename = f"xiaohongshu_hot_search_{today}"
        
        # åˆ›å»ºæ•°æ®ä¿å­˜å™¨å¹¶è®¾ç½®æ–°çš„ç›®å½•ç»“æ„
        saver = DataSaver()
        
        # åˆ›å»ºä¼šè¯ç›®å½•ç»“æ„: ymd/time/
        ymd = datetime.now().strftime("%Y%m%d")
        time_str = datetime.now().strftime("%H%M")
        session_dir = saver.create_session_directory(ymd, time_str)
        
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {session_dir}")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼åˆ°crawl_dataå­ç›®å½•
        json_path = saver.save_json(result, f"{data_filename}.json", save_to_crawl_data=True)
        if json_path:
            print(f"ğŸ“„ JSONæ•°æ®å·²ä¿å­˜åˆ°: {json_path}")
        
        # ä¿å­˜ä¸ºCSVæ ¼å¼åˆ°crawl_dataå­ç›®å½•
        csv_path = saver.save_csv(result, f"{data_filename}.csv", save_to_crawl_data=True)
        if csv_path:
            print(f"ğŸ“Š CSVæ•°æ®å·²ä¿å­˜åˆ°: {csv_path}")
        
        # æ•°æ®åˆ†æ
        print("æ­£åœ¨åˆ†æçƒ­æœæ•°æ®...")
        analysis_result = analyze_hot_search_data(result)
        
        if analysis_result:
            print("âœ… æ•°æ®åˆ†æå®Œæˆ")
            print(f"ğŸ“ˆ å¹³å‡çƒ­åº¦: {analysis_result.get('avg_heat', 0):,.0f}")
            print(f"ğŸ”¥ æœ€é«˜çƒ­åº¦: {analysis_result.get('max_heat', 0):,.0f}")
            print(f"ğŸ“Š åˆ†ç±»æ•°é‡: {len(analysis_result.get('categories', {}))}")
        else:
            print("âš ï¸  æ•°æ®åˆ†æå¤±è´¥")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        print("æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        report_path = generate_html_report(
            result, 
            analysis_result, 
            "simple_analysis.html",
            output_dir=session_dir
        )
        
        if report_path:
            print(f"ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        else:
            print("âš ï¸  æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
        
        # æ˜¾ç¤ºå‰10æ¡çƒ­æœ
        print("\nğŸ“ˆ ä»Šæ—¥çƒ­æœæ¦œ TOP 10:")
        print("-" * 50)
        for item in result.get_top_n(10):
            print(f"{item.rank:2d}. {item.word}")
            if item.heat > 0:
                print(f"    ğŸ”¥ çƒ­åº¦: {item.heat:,}")
            if hasattr(item, 'category') and item.category:
                print(f"    ğŸ·ï¸  åˆ†ç±»: {item.category}")
            print()
        
        return 0
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        return 1
    finally:
        if 'crawler' in locals():
            crawler.close()
        print("\nâœ¨ çˆ¬è™«ä»»åŠ¡ç»“æŸ")


if __name__ == "__main__":
    exit(main())